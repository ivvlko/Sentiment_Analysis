{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is to test some of the functionality of our main project as well as our exported models. We'll start with the last since it's much more intriguing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the prepared/pickled models.\n",
    "\n",
    "All we need here are three imports. Pandas to read the new data, pickle to load the binary field and one import from the python file for our custom pre-processing functions. Actually all we need to do is to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vlado/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from functions import custom_clean_transformer, custom_stem_transformer, clean_text, stem_series\n",
    "from sklearn.metrics import f1_score\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the new data. Here we basically skip the first 80k rows -\n",
    "# the observation we worked with in our main notebook.The other 83k are completely\n",
    "# new, unfamiliar to our models.\n",
    "\n",
    "data = pd.read_csv('data/Twitter_Data.csv', skiprows=80000)\n",
    "data.columns = ['review', 'sentiment']\n",
    "data = data.drop_duplicates()\n",
    "data = data.dropna()\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the saved binary objects\n",
    "\n",
    "classifier_f = open(\"logistic_regression.pickle\", \"rb\")\n",
    "logistic_regression = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_f = open(\"linear_svm.pickle\", \"rb\")\n",
    "linear_svm = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_f = open(\"ensemble.pickle\", \"rb\")\n",
    "ensemble = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how you read the binary, pickled object we saved. \n",
    "\n",
    "Actually this is it. We opened the pickled file, we loaded the two functions that we wrote to process the new data and we are ready to go.  We can now use our model here with our plain new data and start doing the familiar operations. You can use it in Jupyter, Terminal, Pycharm, VSC, anywhere with python code.\n",
    "\n",
    "Let's start playing around with our new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 82.59009280462818%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Logistic Regression accuracy: {logistic_regression.score(data['review'], data['sentiment']) *100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM accuracy: 82.58406653007111%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear SVM accuracy: {linear_svm.score(data['review'], data['sentiment']) *100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble accuracy: 83.8965891286007%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ensemble accuracy: {ensemble.score(data['review'], data['sentiment']) *100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1: 81.23414763582463%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Logistic Regression F1: {f1_score(logistic_regression.predict(data['review']), data['sentiment'], average='macro') *100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM F1: 81.46042108758674%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear SVM F1: {f1_score(linear_svm.predict(data['review']), data['sentiment'], average='macro') *100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble F1: 82.64814419561911%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ensemble F1: {f1_score(ensemble.predict(data['review']), data['sentiment'], average='macro') *100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our models with some 'tweets' written by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = ['i do not care about him',\n",
    "             'modi is the worst prime we have had i hope he goes straight to prison when he is done', \n",
    "             'we will never find such responsible, successful prime like him',\n",
    "             'i am running out of ideas',\n",
    "             'He sucks big time he needs to go now terrible ruler',\n",
    "             'if you expect to find better leader than him - you are lying yourself'\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16552075 0.81231821 0.02216103]\n",
      " [0.97187613 0.00835537 0.0197685 ]\n",
      " [0.00487309 0.00340679 0.99172012]\n",
      " [0.06341912 0.87423121 0.06234967]\n",
      " [0.96287604 0.0349999  0.00212406]\n",
      " [0.00416387 0.00436125 0.99147488]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  1,  0, -1,  1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(logistic_regression.predict_proba(my_tweets))\n",
    "logistic_regression.predict(my_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the predicted classes and the probabily above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  1,  0, -1,  1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Linear SVM doesn't support predict probability but we've got predict as well\n",
    "linear_svm.predict(my_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07654853 0.89387517 0.0295763 ]\n",
      " [0.97783484 0.00247466 0.0196905 ]\n",
      " [0.00646574 0.00335635 0.99017791]\n",
      " [0.03575313 0.92797214 0.03627473]\n",
      " [0.90796465 0.08350527 0.00853008]\n",
      " [0.00736976 0.00415462 0.98847562]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  1,  0, -1,  1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ensemble.predict_proba(my_tweets))\n",
    "ensemble.predict(my_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gave him pretty easy tweets and it guessed correctly all of them.Feel free to change the list and test it yourself. \n",
    "\n",
    "It's a good thing that we achieved the moment when we only load our model, give him raw data and tell him to predict and it does all the things behind - from cleaning and stemming, through the bag of words and tf-idf and finally making the decision.As you can see, we are in completely new environment, we don't have anything imported other than the two libraries and we could have easily skip the pandas import and give him raw scrapped data from twitter for example or whatever. We just confirmed it works - with the same dataset but with unfamiliar observations, it achieved score very close to the original ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests\n",
    "\n",
    "Here we'll be checking the functionality of the code we wrote ourselves in the main notebook. Mainly the pre-processing functions 'clean_text' and 'stem_series'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_cleaning_function_with_dirty_data_should_return_cleaned_lower_case (__main__.FunctionsTests) ... ok\n",
      "test_cleaning_function_with_dirty_numericData_should_return_cleaned_lower_case (__main__.FunctionsTests) ... ok\n",
      "test_cleaning_function_with_onlyJunk_should_return_empty (__main__.FunctionsTests) ... ok\n",
      "test_cleaning_function_with_symbols_should_return_cleaned_lower_case (__main__.FunctionsTests) ... ok\n",
      "test_cleaning_function_with_whiteSpaces_should_return_cleaned_lower_case (__main__.FunctionsTests) ... ok\n",
      "test_cleaning_with_cleaned_should_return_same (__main__.FunctionsTests) ... ok\n",
      "test_stemming_function_different_type_words_should_stem_and_return_stemmed (__main__.FunctionsTests) ... ok\n",
      "test_stemming_function_with_list_with_similar_words_should_return_list_with_one_repeating_word (__main__.FunctionsTests) ... ok\n",
      "test_stemming_with_already_stemmedList_should_return_the_same_list (__main__.FunctionsTests) ... ok\n",
      "test_stemming_with_empty_should_return_empty (__main__.FunctionsTests) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 10 tests in 0.009s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7efef5d17220>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FunctionsTests(unittest.TestCase):\n",
    "    \n",
    "    def test_cleaning_function_with_dirty_numericData_should_return_cleaned_lower_case(self):\n",
    "        to_be_given = \"1234MyName is dumb\"\n",
    "        actual = clean_text(to_be_given)\n",
    "        expected = 'myname is dumb'\n",
    "        self.assertEqual(actual, expected)\n",
    "        \n",
    "    \n",
    "    def test_cleaning_function_with_dirty_data_should_return_cleaned_lower_case(self):\n",
    "        to_be_given = \"$#@$!@#$MyName is dumb\"\n",
    "        actual = clean_text(to_be_given)\n",
    "        expected = 'myname is dumb'\n",
    "        self.assertEqual(actual, expected)\n",
    "    \n",
    "    \n",
    "    def test_cleaning_function_with_whiteSpaces_should_return_cleaned_lower_case(self):\n",
    "        to_be_given = \"   Myname is dumb   \"\n",
    "        actual = clean_text(to_be_given)\n",
    "        expected = 'myname is dumb'\n",
    "        self.assertEqual(actual, expected)\n",
    "        \n",
    "    \n",
    "    def test_cleaning_function_with_symbols_should_return_cleaned_lower_case(self):\n",
    "        to_be_given = \"Hi I'd like to introduce myself. My name's Ivan\"\n",
    "        actual = clean_text(to_be_given)\n",
    "        expected = 'hi i would like to introduce myself my name ivan'\n",
    "        self.assertEqual(actual, expected)\n",
    "        \n",
    "    \n",
    "    def test_cleaning_with_cleaned_should_return_same(self):\n",
    "        to_be_given = 'hi i am ivan'\n",
    "        actual = clean_text(to_be_given)\n",
    "        expected = to_be_given\n",
    "        self.assertEqual(expected, actual)\n",
    "        \n",
    "    \n",
    "    def test_cleaning_function_with_onlyJunk_should_return_empty(self):\n",
    "        to_be_given = '21393921 %3!#@*913!@#*    $!#@*!@#(#!)'\n",
    "        actual = clean_text(to_be_given)\n",
    "        expected = \"\"\n",
    "        self.assertEqual(actual, expected)\n",
    "        \n",
    "    \n",
    "    def test_stemming_function_with_list_with_similar_words_should_return_list_with_one_repeating_word(self):\n",
    "        to_be_given = ['playing', 'plays', \"playful\"]\n",
    "        actual = stem_series(to_be_given)\n",
    "        expected = [\"play \"] * len(to_be_given)\n",
    "        self.assertEqual(actual, expected)\n",
    "        \n",
    "        \n",
    "    def test_stemming_function_different_type_words_should_stem_and_return_stemmed(self):\n",
    "        to_be_given = ['television', 'computer']\n",
    "        actual = stem_series(to_be_given)\n",
    "        expected = ['televis ', 'comput ']\n",
    "        self.assertEqual(actual, expected)\n",
    "        \n",
    "        \n",
    "    def test_stemming_with_already_stemmedList_should_return_the_same_list(self):\n",
    "        to_be_given = ['televi', \"comput\"]\n",
    "        actual = stem_series(to_be_given)\n",
    "        expected = ['televi ', 'comput ']\n",
    "        self.assertEqual(actual, expected)\n",
    "    \n",
    "    def test_stemming_with_empty_should_return_empty(self):\n",
    "        to_be_given = []\n",
    "        actual = stem_series(to_be_given)\n",
    "        expected = []\n",
    "        self.assertEqual(actual, expected)\n",
    "        \n",
    "        \n",
    "    \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
